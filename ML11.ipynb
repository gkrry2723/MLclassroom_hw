{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML11.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPBjHVmDu8CPE4VyEwsMZgv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gkrry2723/MLclassroom_hw/blob/main/ML11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSFCksybD00D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "a22f9dc1-dc70-4504-b98f-ab1c7ba2eec2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/gdrive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDqKKeWpDivI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "2a978eab-6d10-4459-e292-514d9e83aec4"
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.datasets import load_files\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "import pickle\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.tag import pos_tag\n",
        "from collections import Counter\n",
        "import collections\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "review_data = load_files(r\"/gdrive/My Drive/ML_assignment/movie_review\")\n",
        "#   X: 문서, y: lable(1: positive, 0: negative) \n",
        "X, y = review_data.data, review_data.target\n",
        "documents = []\n",
        "pluss=[]\n",
        "\n",
        "stemmer = WordNetLemmatizer()\n",
        "stemmer2 = nltk.stem.SnowballStemmer('english')\n",
        "for sen in range(0, len(X)):\n",
        "    # Remove all the special characters\n",
        "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
        "    \n",
        "    # remove all single characters\n",
        "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "    \n",
        "    # Remove single characters from the start\n",
        "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
        "    \n",
        "    # Substituting multiple spaces with single space\n",
        "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "    \n",
        "    # Removing prefixed 'b'\n",
        "    document = re.sub(r'^b\\s+', '', document)\n",
        "    \n",
        "    # Converting to Lowercase\n",
        "    document = document.lower()\n",
        "    \n",
        "    # Lemmatization\n",
        "    document = document.split()\n",
        "    # worst_num=document.count(\"worst\")\n",
        "    # boring_num=document.count(\"boring\")\n",
        "\n",
        "    #tags_en = nltk.pos_tag(document)\n",
        "    #adj_list = [t[0] for t in tags_en if t[1] == \"JJ\" or t[1] == \"JJR\" or t[1]==\"JJS\" or t[1]==\"RBR\"]\n",
        "    \n",
        "    document = [stemmer.lemmatize(word) for word in document]\n",
        "    document = ' '.join(document)\n",
        "    documents.append(document)\n",
        "\n",
        "vectorizer = CountVectorizer(min_df=10, max_df=0.8, stop_words=stopwords.words('english'))\n",
        "#vectorizer = CountVectorizer(min_df=5,max_df=0.8, stop_words=stopwords.words('english'))\n",
        "X = vectorizer.fit_transform(documents).toarray()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuieM3mYXqV7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "76ac8e32-8a72-4af3-bc59-206b27883a68"
      },
      "source": [
        "list_x=X.tolist()\n",
        "negative=[]\n",
        "positive=[]\n",
        "for i in range(0,2002):\n",
        "  if y[i]==0:\n",
        "    negative.append(list_x[i])\n",
        "  else:\n",
        "    positive.append(list_x[i])\n",
        "negative=np.array(negative)\n",
        "positive=np.array(positive)\n",
        "total_sum=X.sum(axis=0)\n",
        "neg_sum=negative.sum(axis=0)\n",
        "pos_sum=positive.sum(axis=0)\n",
        "\n",
        "neg_ratio=neg_sum/total_sum\n",
        "pos_ratio=pos_sum/total_sum\n",
        "\n",
        "good=[]\n",
        "for i in range(0,len(pos_sum)):\n",
        "  if neg_ratio[i]>pos_ratio[i]*3:\n",
        "    good.append(i)\n",
        "  elif pos_ratio[i]>neg_ratio[i]*3:\n",
        "    good.append(i)\n",
        "print(\"good의 길이:   \" , len(good))\n",
        "new=np.ones((1,2002), dtype=float)\n",
        "for i in range(0,len(good)):\n",
        "  temp=X[:,good[i]]\n",
        "  temp=temp[np.newaxis]\n",
        "  new=np.vstack((new,temp))\n",
        "new=np.transpose(new)\n",
        "new=new[:,1:]\n",
        "# vectorizer = CountVectorizer(max_features=2, min_df=0, stop_words=stopwords.words('english'))\n",
        "# Xp = vectorizer.fit_transform(pluss).toarray()\n",
        "\n",
        "# tfidfconverter = TfidfTransformer()\n",
        "# X = tfidfconverter.fit_transform(X).toarray()\n",
        "X_train, X_test, y_train, y_test = train_test_split(new, y, test_size=0.3, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "good의 길이:    935\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSXVlNV2a0Af"
      },
      "source": [
        "import math\n",
        "train_x=X_train\n",
        "test_x=X_test\n",
        "train_l=y_train\n",
        "test_l=y_test\n",
        "\n",
        "train_x=np.transpose(train_x)\n",
        "test_x=np.transpose(test_x)\n",
        "\n",
        "# 1. 세타 값 initialization\n",
        "mu, sigma = 0, 0.1\n",
        "u=np.random.normal(mu,sigma,(300,len(train_x)+1))\n",
        "v=np.random.normal(mu,sigma,(60,301))\n",
        "w=np.random.normal(mu,sigma,(12,61))\n",
        "t=np.random.normal(mu,sigma,(13))[np.newaxis]\n",
        "\n",
        "#bias 값 추가\n",
        "temp1=np.ones((len(train_x[0])), dtype=float)\n",
        "temp2=np.ones(len((test_x[0])), dtype=float)\n",
        "\n",
        "train_x=np.insert(train_x,0,temp1,axis=0)\n",
        "test_x=np.insert(test_x,0,temp2,axis=0)\n",
        "\n",
        "#plot위한 list\n",
        "J_train=[]\n",
        "accurate_train=[]\n",
        "steps=[]\n",
        "\n",
        "J_test=[]\n",
        "accurate_test=[]\n",
        "\n",
        "#sigmoid 함수 이후 값\n",
        "y_s = np.zeros((301, len(train_x[0])), dtype=float)\n",
        "z_s = np.zeros((61,len(train_x[0])), dtype=float)\n",
        "k_s = np.zeros((13,len(train_x[0])), dtype=float)\n",
        "h_s = np.zeros((1, len(train_x[0])), dtype=float)\n",
        "\n",
        "y_st = np.zeros((301, len(test_x[0])), dtype=float)\n",
        "z_st = np.zeros((61,len(test_x[0])), dtype=float)\n",
        "k_st = np.zeros((13,len(test_x[0])), dtype=float)\n",
        "h_st = (np.zeros((len(test_x[0])), dtype=float))[np.newaxis]\n",
        "\n",
        "#label위한 diagonal 행렬\n",
        "#l=np.identity(10)\n",
        "\n",
        "#bias 값 추가(sigmoid layer에만)\n",
        "for i in range(0,len(train_x[0])):\n",
        "  if(i<len(test_x[0])):\n",
        "    y_st[0][i]=1\n",
        "    z_st[0][i]=1\n",
        "  y_s[0][i]=1\n",
        "  z_s[0][i]=1\n",
        "  \n",
        " \n",
        "for c in range(0,4000):\n",
        "  steps.append(c)\n",
        "\n",
        "  # 3. J 구하기\n",
        "  #3-1 y랑 sigmoid y\n",
        "  y=np.dot(u,train_x)\n",
        "  yt=np.dot(u,test_x)\n",
        "  for a in range(1,301):\n",
        "    for b in range(0,len(train_x[0])):\n",
        "      if(b<len(test_x[0])):\n",
        "        y_st[a][b]=1/(1+math.exp(-yt[a-1][b]))\n",
        "      y_s[a][b]=1/(1+math.exp(-y[a-1][b]))\n",
        "\n",
        "  #3-2 z 랑 sigmoid z\n",
        "  z=np.dot(v,y_s)\n",
        "  zt=np.dot(v,y_st)\n",
        "  for a in range(1,61):\n",
        "    for b in range(0,len(train_x[0])):\n",
        "      if(b<len(test_x[0])):\n",
        "        z_st[a][b]=1/(1+math.exp(-zt[a-1][b]))\n",
        "      z_s[a][b]=1/(1+math.exp(-z[a-1][b]))\n",
        "  \n",
        "  #3-3 k 랑 sigmoid k\n",
        "  k=np.dot(w,z_s)\n",
        "  kt=np.dot(w,z_st)\n",
        "  for a in range(1,13):\n",
        "    for b in range(0,len(train_x[0])):\n",
        "      if(b<len(test_x[0])):\n",
        "        k_st[a][b]=1/(1+math.exp(-kt[a-1][b]))\n",
        "      k_s[a][b]=1/(1+math.exp(-k[a-1][b]))\n",
        "\n",
        "  #3-4 H랑 sigmoid h\n",
        "  h=np.dot(t,k_s)\n",
        "  ht=np.dot(t,k_st)\n",
        "  for b in range(0,len(train_x[0])):\n",
        "    if(b<len(test_x[0])):\n",
        "      h_st[0][b]=1/(1+math.exp(-ht[0][b]))\n",
        "    h_s[0][b]=1/(1+math.exp(-h[0][b]))\n",
        "\n",
        "  #5. gradient descent 하기, J 구하기, accurate 세기\n",
        "  J_sum=0\n",
        "  theta_sum=0\n",
        "  p=0.2\n",
        "  lambdaa=2\n",
        "\n",
        "  h_s_t=np.transpose(h_s)\n",
        "  k_s_t=np.transpose(k_s[1:,:])\n",
        "  z_s_t=np.transpose(z_s[1:,:])\n",
        "  y_s_t=np.transpose(y_s[1:,:])\n",
        "  x_s_t=np.transpose(train_x[1:,:])\n",
        "\n",
        "  t_t=t[:,1:]\n",
        "  w_t=w[:,1:]\n",
        "  v_t=v[:,1:]\n",
        "  u_t=u[:,1:]\n",
        "  \n",
        "  one_k=np.transpose(np.ones((12), dtype=float))\n",
        "  one_z=np.transpose(np.ones((60), dtype=float))\n",
        "  one_y=np.transpose(np.ones((300), dtype=float))\n",
        "\n",
        "  delta5=np.transpose((np.zeros((12), dtype=float)))\n",
        "  delta4=np.zeros((60,12), dtype=float)\n",
        "  delta3=np.zeros((300,60), dtype=float)\n",
        "  delta2=np.zeros((len(train_x)-1,300), dtype=float)\n",
        "\n",
        "  Jt_sum=0\n",
        "\n",
        "  y_pred_test=np.empty(len(test_x[0]), dtype=int)\n",
        "  y_pred_train=np.empty(len(train_x[0]), dtype=int)\n",
        "  count_test=0\n",
        "  count=0\n",
        "\n",
        "  for i in range(0,len(train_x[0])):\n",
        "    if(h_s[0][i]>=0.5):\n",
        "      result=1\n",
        "      y_pred_train[i]=1 \n",
        "    else:\n",
        "      result=0\n",
        "      y_pred_train[i]=0\n",
        "    if(result==train_l[i]):\n",
        "      count=count+1\n",
        "    \n",
        "\n",
        "    lable=train_l[i]\n",
        "    temp5=h_s_t[i]-train_l[i]\n",
        "    temp4=np.multiply( np.dot( np.transpose( t_t ) , temp5 ) ,np.transpose(np.multiply(k_s_t[i] , one_k - k_s_t[i])))\n",
        "    temp3=np.multiply( np.dot( np.transpose(w_t) , temp4) , np.transpose(np.multiply(z_s_t[i] , one_z - z_s_t[i])))\n",
        "    temp2=np.multiply( np.dot( np.transpose( v_t ) , temp3) , np.transpose(np.multiply(y_s_t[i] , one_y - y_s_t[i])))\n",
        "\n",
        "    delta5=delta5+ np.dot( np.transpose(k_s_t[i][np.newaxis]),temp5[np.newaxis])\n",
        "    delta4=delta4 + np.dot( np.transpose(z_s_t[i][np.newaxis]), temp4[np.newaxis])\n",
        "    delta3=delta3+ np.dot( np.transpose(y_s_t[i][np.newaxis]), temp3[np.newaxis] )\n",
        "    delta2=delta2+ np.dot(np.transpose(x_s_t[i][np.newaxis]), temp2[np.newaxis])\n",
        "\n",
        "    J_sum=J_sum+(-train_l[i]*math.log(h_s[0][i])-(1-train_l[i])*math.log(1-h_s[0][i]))\n",
        "\n",
        "    #######################test\n",
        "    if i<len(test_x[0]):\n",
        "      if(h_st[0][i]>=0.5):\n",
        "        y_pred_test[i]=1\n",
        "      else:\n",
        "        y_pred_test[i]=0\n",
        "      if(y_pred_test[i]==test_l[i]):\n",
        "        count_test=count_test+1\n",
        "\n",
        "      \n",
        "      Jt_sum=Jt_sum+(-test_l[i]*math.log(h_st[0][i])-(1-test_l[i])*math.log(1-h_st[0][i]))\n",
        "    ###########################\n",
        "\n",
        "  u2=np.multiply(u_t,u_t)\n",
        "  v2=np.multiply(v_t,v_t)\n",
        "  w2=np.multiply(w_t,w_t)\n",
        "  t2=np.multiply(t_t,t_t)\n",
        "  theta_sum=np.sum(u2)+np.sum(v2)+np.sum(w2)+np.sum(t2)\n",
        "\n",
        "  accuracy=count/len(train_x[0])\n",
        "  accurate_train.append(accuracy)\n",
        "\n",
        "  delta5=delta5/len(train_x[0])+lambdaa*np.transpose(t_t)/(12+12*60+60*300+300*(len(train_x)-1))\n",
        "  delta4=delta4/len(train_x[0])+lambdaa*np.transpose(w_t)/(12+12*60+60*300+300*(len(train_x)-1))\n",
        "  delta3=delta3/len(train_x[0])+lambdaa*np.transpose(v_t)/(12+12*60+60*300+300*(len(train_x)-1))\n",
        "  delta2=delta2/len(train_x[0])+lambdaa*np.transpose(u_t)/(12+12*60+60*300+300*(len(train_x)-1))\n",
        "\n",
        "  J_sum=J_sum/len(train_x[0])+lambdaa*theta_sum/(2*(12+12*60+60*300+300*(len(train_x)-1)))\n",
        "  J_train.append(J_sum)\n",
        "\n",
        "  ####################test\n",
        "  accuracy_test=count_test/len(test_x[0])\n",
        "  accurate_test.append(accuracy_test)\n",
        "\n",
        "  Jt_sum=Jt_sum/len(test_x[0])+lambdaa*theta_sum/(2*(12+12*60+60*300+300*(len(train_x)-1)))\n",
        "  J_test.append(Jt_sum)\n",
        "  ######################\n",
        "\n",
        "  for a in range(0,300):\n",
        "    for b in range(1,len(train_x)):\n",
        "      if a<1 and b<13:\n",
        "        t[a][b]=t[a][b]-p*(np.transpose(delta5)[a][b-1])\n",
        "      if a<12 and b<61:\n",
        "        w[a][b]=w[a][b]-p*(np.transpose(delta4)[a][b-1])\n",
        "      if a<60 and b<301:\n",
        "        v[a][b]=v[a][b]-p*np.transpose(delta3)[a][b-1]\n",
        "      u[a][b]=u[a][b]-p*np.transpose(delta2)[a][b-1]\n",
        "\n",
        "  print(c,\"train:::acccuracy  \",accuracy,\"   J   \",J_sum,\"::::: test accuracy\",accuracy_test,\"   J   \",Jt_sum)\n",
        "  # if accuracy_test>=0.89:\n",
        "  #   break;\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "print(confusion_matrix(y_train,y_pred_train))\n",
        "print(classification_report(y_train,y_pred_train))\n",
        "print(accuracy_score(y_train, y_pred_train))\n",
        "\n",
        "print(confusion_matrix(y_test,y_pred_test))\n",
        "print(classification_report(y_test,y_pred_test))\n",
        "print(accuracy_score(y_test, y_pred_test))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttf0TFcDEnV0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 답 쓰는 곳\n",
        "print(\"\\033[1m\",\"\\033[31m\",\"***************정답 20184754 김현주***************\",\"\\033[0m\",\"\\n\")\n",
        "print(\"\\033[1m\",\"1. Plot the loss curve\",\"\\033[0m\")\n",
        "f1 = plt.figure(1) \n",
        "plt.scatter(steps,J_train,c=\"blue\",s=0.3,label='train')\n",
        "plt.scatter(steps,J_test,c=\"red\",s=0.3,label='test')\n",
        "\n",
        "plt.xlabel('step')\n",
        "plt.ylabel('Cost')\n",
        "\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "plt.show() \n",
        "\n",
        "print(\"\\033[1m\",\"2.  Plot the accuracy curve\",\"\\033[0m\")\n",
        "f2 = plt.figure(2)\n",
        "plt.scatter(steps,accurate_train,c=\"blue\",s=0.3,label='train')\n",
        "plt.scatter(steps,accurate_test,c=\"red\",s=0.3,label='test')\n",
        "\n",
        "plt.xlabel('step')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "plt.show() \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALcFyprVG-zo"
      },
      "source": [
        "print(\"\\033[1m\",\"3.  Plot the quantitative results\",\"\\033[0m\")\n",
        "print(\"\\033[31m\",confusion_matrix(y_train,y_pred_train))\n",
        "print(classification_report(y_train,y_pred_train))\n",
        "print(accuracy_score(y_train, y_pred_train),\"\\033[0m\")\n",
        "\n",
        "print(\"\\033[34m\",confusion_matrix(y_test,y_pred_test))\n",
        "print(classification_report(y_test,y_pred_test))\n",
        "print(\"\\033[34m\",accuracy_score(y_test, y_pred_test),\"\\033[0m\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}